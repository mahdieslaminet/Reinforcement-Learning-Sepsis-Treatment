import numpy as np
import torch
from sklearn.ensemble import RandomForestClassifier
from tqdm import tqdm

def compute_action_probs_from_agent(agent, states):
    """
    Given an agent and states (T,F numpy), return probs (T, n_actions).
    """
    agent.model.eval()
    with torch.no_grad():
        s = torch.tensor(states, dtype=torch.float32, device=agent.device)
        logits, _ = agent.model(s)
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
    return probs

def fit_behavior_model(trajs):
    """
    Fit a RandomForest classifier to predict clinicians' actions. Returns a callable that
    maps states -> action probs. This replicates the paper's clinician-policy modeling.
    """
    X = np.concatenate([t['states'] for t in trajs], axis=0)
    y = np.concatenate([t['actions'] for t in trajs], axis=0)
    rf = RandomForestClassifier(n_estimators=200, n_jobs=-1)
    rf.fit(X, y)
    def prob_fn(states):
        return rf.predict_proba(states)  # columns correspond to rf.classes_
    return rf, prob_fn

def wis_estimate(agent, trajs, behavior_prob_fn=None, n_bootstrap=1000, seed=0):
    """
    Compute weighted IS estimate of average episode return under agent's policy,
    using trajectories generated by behavior policy (clinicians).

    Steps:
      - for each trajectory compute per-step likelihood ratio: pi_e(a|s)/pi_b(a|s)
      - compute per-trajectory weight = product of likelihood ratios
      - WIS estimate = sum(weight * return) / sum(weight)

    Because products can underflow, compute in log-space.
    """
    np.random.seed(seed)
    agent.model.eval()
    # get agent action probs for all steps
    all_returns = []
    all_weights = []
    for t in tqdm(trajs, desc="WIS per traj"):
        states = t['states']
        actions = t['actions']
        probs_e = compute_action_probs_from_agent(agent, states)  # shape (T, K)
        if behavior_prob_fn is None:
            raise ValueError("behavior_prob_fn (clinician probs) required")
        probs_b = behavior_prob_fn(states)  # shape (T, K) or list of classes
        # ensure alignment of columns: sklearn's predict_proba order might not be 0..K-1
        # here we assume classes are 0..K-1.
        # compute log weight
        log_w = 0.0
        for i,a in enumerate(actions):
            pe = max(1e-12, probs_e[i, a])
            pb = max(1e-12, probs_b[i, a])
            log_w += np.log(pe) - np.log(pb)
        w = np.exp(log_w)
        returns = float(t['rewards'].sum())  # episodic return
        all_weights.append(w)
        all_returns.append(returns)
    all_weights = np.array(all_weights)
    all_returns = np.array(all_returns)
    if all_weights.sum() == 0:
        print("All weights zero (numerical underflow). Try stabilized IS or evaluation on subset.")
        return None
    wis = (all_weights * all_returns).sum() / all_weights.sum()

    # bootstrap to get 95% lower bound
    boot_estimates = []
    N = len(all_returns)
    for _ in range(n_bootstrap):
        idx = np.random.randint(0, N, size=N)
        w_b = all_weights[idx]
        r_b = all_returns[idx]
        if w_b.sum() == 0:
            boot_estimates.append(0.0)
        else:
            boot_estimates.append((w_b * r_b).sum() / w_b.sum())
    lower95 = np.percentile(boot_estimates, 2.5)
    return {'wis': wis, 'bootstrap_estimates': boot_estimates, 'lower95': lower95}

if __name__ == "__main__":
    print("Run eval_offpolicy.wis_estimate() from eval script after training.")
